{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GraphSAGE: Inductive Representation Learning on Large Graphs\n",
    "\n",
    "**GraphSAGE**是一个用于大型图上归纳表示学习的框架。\n",
    "\n",
    "**GraphSAGE**用于为节点生成低维向量表示, 对于具有丰富节点属性信息的图尤其有用。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.7.1+cu101 \n",
      "numpy version: 1.18.2 \n",
      "matplotlib version: 3.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('pytorch version:',torch.__version__,\n",
    "      '\\nnumpy version:' ,np.__version__,\n",
    "      '\\nmatplotlib version:' ,matplotlib.__version__)\n",
    "\n",
    "# (可选) 适配项目路径\n",
    "import sys\n",
    "sys.path.insert(0, \"/root/workshop/Deep-Learning-in-Action\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining hyperparameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "INPUT_DIM = 1433 # 输入维度\n",
    "# Note: 采样的邻居阶数需要与GCN的层数保持一致\n",
    "HIDDEN_DIM = [128, 7]   # 隐藏单元节点数\n",
    "NUM_NEIGHBORS_LIST = [10, 10]   # 每阶采样邻居的节点数\n",
    "assert len(HIDDEN_DIM) == len(NUM_NEIGHBORS_LIST)\n",
    "\n",
    "BATCH_SIZE = 16 # 批处理大小\n",
    "EPOCHS = 20\n",
    "NUM_BATCH_PER_EPOCH = 20 # 每个epoch循环的批次数\n",
    "LEARNING_RATE = 0.01 # 学习率\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Dataset: Cora"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process data ...\n",
      "Node's feature shape:  (2708, 1433)\n",
      "Node's label shape:  (2708,)\n",
      "Adjacency's shape:  2708\n",
      "Number of training nodes:  140\n",
      "Number of validation nodes:  500\n",
      "Number of test nodes:  1000\n",
      "Cached file: /root/data/cora/raw/ch7_cached.pkl\n"
     ]
    }
   ],
   "source": [
    "from graph_neural_networks.GraphSage.cora import CoraData\n",
    "from collections import namedtuple\n",
    "\n",
    "Data = namedtuple('Data', ['x', 'y', 'adjacency_dict',\n",
    "                           'train_mask', 'val_mask', 'test_mask'])\n",
    "\n",
    "# 加载数据，并转换为torch.Tensor\n",
    "dataset = CoraData(data_root='/root/data/cora/raw', rebuild=True).data\n",
    "x = dataset.x / dataset.x.sum(1, keepdims=True)  # 归一化数据，使得每一行和为1\n",
    "train_idx = np.where(dataset.train_mask)[0]\n",
    "train_label = dataset.y\n",
    "test_idx = np.where(dataset.test_mask)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build GraphSage, define optimizer and loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "from graph_neural_networks.GraphSage.model import GraphSage\n",
    "\n",
    "model = GraphSage(input_dim=INPUT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "                  num_neighbors_list=NUM_NEIGHBORS_LIST).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define training function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "from graph_neural_networks.GraphSage.sampling import multihop_sampling\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for e in range(EPOCHS):\n",
    "        for batch in range(NUM_BATCH_PER_EPOCH):\n",
    "            bth_src_idx = np.random.choice(train_idx, size=(BATCH_SIZE,))\n",
    "            bth_src_label = torch.from_numpy(train_label[bth_src_idx]).long().to(DEVICE)\n",
    "            bth_sampling_res = multihop_sampling(bth_src_idx, NUM_NEIGHBORS_LIST, dataset.adjacency_dict)\n",
    "            bth_sampling_x = [torch.from_numpy(x[idx]).float().to(DEVICE) for idx in bth_sampling_res]\n",
    "\n",
    "            bth_train_logits = model(bth_sampling_x)\n",
    "            loss = criterion(bth_train_logits, bth_src_label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(\"Epoch {:03d} Batch {:03d} Loss: {:.4f}\".format(e, batch, loss.item()))\n",
    "        test()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_sampling_result = multihop_sampling(test_idx, NUM_NEIGHBORS_LIST, dataset.adjacency_dict)\n",
    "    test_x = [torch.from_numpy(x[idx]).float().to(DEVICE) for idx in test_sampling_result]\n",
    "    test_label = torch.from_numpy(dataset.y[test_idx]).long().to(DEVICE)\n",
    "\n",
    "    test_logits = model(test_x)\n",
    "    predict_y = test_logits.max(1)[1]\n",
    "    accuarcy = torch.eq(predict_y, test_label).float().mean().item()\n",
    "    print(\"--------------------------------\")\n",
    "    print(\"Test Accuracy: \", accuarcy)\n",
    "    print(\"--------------------------------\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 Batch 000 Loss: 1.9836\n",
      "Epoch 000 Batch 001 Loss: 1.9199\n",
      "Epoch 000 Batch 002 Loss: 1.8195\n",
      "Epoch 000 Batch 003 Loss: 1.6954\n",
      "Epoch 000 Batch 004 Loss: 1.6667\n",
      "Epoch 000 Batch 005 Loss: 1.4803\n",
      "Epoch 000 Batch 006 Loss: 1.4893\n",
      "Epoch 000 Batch 007 Loss: 1.4632\n",
      "Epoch 000 Batch 008 Loss: 0.9855\n",
      "Epoch 000 Batch 009 Loss: 1.1157\n",
      "Epoch 000 Batch 010 Loss: 0.9806\n",
      "Epoch 000 Batch 011 Loss: 0.7594\n",
      "Epoch 000 Batch 012 Loss: 0.6640\n",
      "Epoch 000 Batch 013 Loss: 0.6778\n",
      "Epoch 000 Batch 014 Loss: 0.7728\n",
      "Epoch 000 Batch 015 Loss: 0.4867\n",
      "Epoch 000 Batch 016 Loss: 0.5257\n",
      "Epoch 000 Batch 017 Loss: 0.3501\n",
      "Epoch 000 Batch 018 Loss: 0.5792\n",
      "Epoch 000 Batch 019 Loss: 0.4938\n",
      "Test Accuracy:  0.7660000324249268\n",
      "Epoch 001 Batch 000 Loss: 0.3622\n",
      "Epoch 001 Batch 001 Loss: 0.3547\n",
      "Epoch 001 Batch 002 Loss: 0.2089\n",
      "Epoch 001 Batch 003 Loss: 0.2336\n",
      "Epoch 001 Batch 004 Loss: 0.1097\n",
      "Epoch 001 Batch 005 Loss: 0.1586\n",
      "Epoch 001 Batch 006 Loss: 0.3172\n",
      "Epoch 001 Batch 007 Loss: 0.0886\n",
      "Epoch 001 Batch 008 Loss: 0.1341\n",
      "Epoch 001 Batch 009 Loss: 0.2027\n",
      "Epoch 001 Batch 010 Loss: 0.2424\n",
      "Epoch 001 Batch 011 Loss: 0.0672\n",
      "Epoch 001 Batch 012 Loss: 0.0497\n",
      "Epoch 001 Batch 013 Loss: 0.2231\n",
      "Epoch 001 Batch 014 Loss: 0.0349\n",
      "Epoch 001 Batch 015 Loss: 0.0788\n",
      "Epoch 001 Batch 016 Loss: 0.0755\n",
      "Epoch 001 Batch 017 Loss: 0.1078\n",
      "Epoch 001 Batch 018 Loss: 0.0921\n",
      "Epoch 001 Batch 019 Loss: 0.1127\n",
      "Test Accuracy:  0.7660000324249268\n",
      "Epoch 002 Batch 000 Loss: 0.0749\n",
      "Epoch 002 Batch 001 Loss: 0.0410\n",
      "Epoch 002 Batch 002 Loss: 0.2399\n",
      "Epoch 002 Batch 003 Loss: 0.0521\n",
      "Epoch 002 Batch 004 Loss: 0.0494\n",
      "Epoch 002 Batch 005 Loss: 0.1144\n",
      "Epoch 002 Batch 006 Loss: 0.0364\n",
      "Epoch 002 Batch 007 Loss: 0.0148\n",
      "Epoch 002 Batch 008 Loss: 0.0208\n",
      "Epoch 002 Batch 009 Loss: 0.0437\n",
      "Epoch 002 Batch 010 Loss: 0.0442\n",
      "Epoch 002 Batch 011 Loss: 0.0495\n",
      "Epoch 002 Batch 012 Loss: 0.0348\n",
      "Epoch 002 Batch 013 Loss: 0.0615\n",
      "Epoch 002 Batch 014 Loss: 0.0620\n",
      "Epoch 002 Batch 015 Loss: 0.0331\n",
      "Epoch 002 Batch 016 Loss: 0.0352\n",
      "Epoch 002 Batch 017 Loss: 0.0362\n",
      "Epoch 002 Batch 018 Loss: 0.0294\n",
      "Epoch 002 Batch 019 Loss: 0.0570\n",
      "Test Accuracy:  0.7930000424385071\n",
      "Epoch 003 Batch 000 Loss: 0.0507\n",
      "Epoch 003 Batch 001 Loss: 0.0434\n",
      "Epoch 003 Batch 002 Loss: 0.0374\n",
      "Epoch 003 Batch 003 Loss: 0.0415\n",
      "Epoch 003 Batch 004 Loss: 0.0354\n",
      "Epoch 003 Batch 005 Loss: 0.0525\n",
      "Epoch 003 Batch 006 Loss: 0.0687\n",
      "Epoch 003 Batch 007 Loss: 0.0536\n",
      "Epoch 003 Batch 008 Loss: 0.0464\n",
      "Epoch 003 Batch 009 Loss: 0.0423\n",
      "Epoch 003 Batch 010 Loss: 0.0314\n",
      "Epoch 003 Batch 011 Loss: 0.0591\n",
      "Epoch 003 Batch 012 Loss: 0.0376\n",
      "Epoch 003 Batch 013 Loss: 0.0602\n",
      "Epoch 003 Batch 014 Loss: 0.0488\n",
      "Epoch 003 Batch 015 Loss: 0.0647\n",
      "Epoch 003 Batch 016 Loss: 0.0429\n",
      "Epoch 003 Batch 017 Loss: 0.0336\n",
      "Epoch 003 Batch 018 Loss: 0.0461\n",
      "Epoch 003 Batch 019 Loss: 0.0669\n",
      "Test Accuracy:  0.7940000295639038\n",
      "Epoch 004 Batch 000 Loss: 0.0424\n",
      "Epoch 004 Batch 001 Loss: 0.0689\n",
      "Epoch 004 Batch 002 Loss: 0.0552\n",
      "Epoch 004 Batch 003 Loss: 0.0545\n",
      "Epoch 004 Batch 004 Loss: 0.0453\n",
      "Epoch 004 Batch 005 Loss: 0.0457\n",
      "Epoch 004 Batch 006 Loss: 0.0574\n",
      "Epoch 004 Batch 007 Loss: 0.0518\n",
      "Epoch 004 Batch 008 Loss: 0.0501\n",
      "Epoch 004 Batch 009 Loss: 0.0491\n",
      "Epoch 004 Batch 010 Loss: 0.0462\n",
      "Epoch 004 Batch 011 Loss: 0.0370\n",
      "Epoch 004 Batch 012 Loss: 0.0792\n",
      "Epoch 004 Batch 013 Loss: 0.0691\n",
      "Epoch 004 Batch 014 Loss: 0.0843\n",
      "Epoch 004 Batch 015 Loss: 0.0445\n",
      "Epoch 004 Batch 016 Loss: 0.0585\n",
      "Epoch 004 Batch 017 Loss: 0.0471\n",
      "Epoch 004 Batch 018 Loss: 0.0516\n",
      "Epoch 004 Batch 019 Loss: 0.0648\n",
      "Test Accuracy:  0.7890000343322754\n",
      "Epoch 005 Batch 000 Loss: 0.0371\n",
      "Epoch 005 Batch 001 Loss: 0.0482\n",
      "Epoch 005 Batch 002 Loss: 0.0486\n",
      "Epoch 005 Batch 003 Loss: 0.0564\n",
      "Epoch 005 Batch 004 Loss: 0.0438\n",
      "Epoch 005 Batch 005 Loss: 0.0462\n",
      "Epoch 005 Batch 006 Loss: 0.0393\n",
      "Epoch 005 Batch 007 Loss: 0.0313\n",
      "Epoch 005 Batch 008 Loss: 0.0575\n",
      "Epoch 005 Batch 009 Loss: 0.0498\n",
      "Epoch 005 Batch 010 Loss: 0.0392\n",
      "Epoch 005 Batch 011 Loss: 0.0737\n",
      "Epoch 005 Batch 012 Loss: 0.0601\n",
      "Epoch 005 Batch 013 Loss: 0.0587\n",
      "Epoch 005 Batch 014 Loss: 0.0602\n",
      "Epoch 005 Batch 015 Loss: 0.0287\n",
      "Epoch 005 Batch 016 Loss: 0.0691\n",
      "Epoch 005 Batch 017 Loss: 0.0431\n",
      "Epoch 005 Batch 018 Loss: 0.0752\n",
      "Epoch 005 Batch 019 Loss: 0.0570\n",
      "Test Accuracy:  0.7700000405311584\n",
      "Epoch 006 Batch 000 Loss: 0.0538\n",
      "Epoch 006 Batch 001 Loss: 0.0568\n",
      "Epoch 006 Batch 002 Loss: 0.0719\n",
      "Epoch 006 Batch 003 Loss: 0.0548\n",
      "Epoch 006 Batch 004 Loss: 0.0665\n",
      "Epoch 006 Batch 005 Loss: 0.0664\n",
      "Epoch 006 Batch 006 Loss: 0.0407\n",
      "Epoch 006 Batch 007 Loss: 0.0574\n",
      "Epoch 006 Batch 008 Loss: 0.0442\n",
      "Epoch 006 Batch 009 Loss: 0.0431\n",
      "Epoch 006 Batch 010 Loss: 0.0672\n",
      "Epoch 006 Batch 011 Loss: 0.0764\n",
      "Epoch 006 Batch 012 Loss: 0.0535\n",
      "Epoch 006 Batch 013 Loss: 0.0469\n",
      "Epoch 006 Batch 014 Loss: 0.0359\n",
      "Epoch 006 Batch 015 Loss: 0.0446\n",
      "Epoch 006 Batch 016 Loss: 0.0461\n",
      "Epoch 006 Batch 017 Loss: 0.0498\n",
      "Epoch 006 Batch 018 Loss: 0.0552\n",
      "Epoch 006 Batch 019 Loss: 0.0647\n",
      "Test Accuracy:  0.7720000147819519\n",
      "Epoch 007 Batch 000 Loss: 0.0434\n",
      "Epoch 007 Batch 001 Loss: 0.0483\n",
      "Epoch 007 Batch 002 Loss: 0.0502\n",
      "Epoch 007 Batch 003 Loss: 0.0403\n",
      "Epoch 007 Batch 004 Loss: 0.0523\n",
      "Epoch 007 Batch 005 Loss: 0.0420\n",
      "Epoch 007 Batch 006 Loss: 0.0592\n",
      "Epoch 007 Batch 007 Loss: 0.0790\n",
      "Epoch 007 Batch 008 Loss: 0.0273\n",
      "Epoch 007 Batch 009 Loss: 0.0369\n",
      "Epoch 007 Batch 010 Loss: 0.0532\n",
      "Epoch 007 Batch 011 Loss: 0.0347\n",
      "Epoch 007 Batch 012 Loss: 0.0329\n",
      "Epoch 007 Batch 013 Loss: 0.0574\n",
      "Epoch 007 Batch 014 Loss: 0.0506\n",
      "Epoch 007 Batch 015 Loss: 0.0565\n",
      "Epoch 007 Batch 016 Loss: 0.0654\n",
      "Epoch 007 Batch 017 Loss: 0.0711\n",
      "Epoch 007 Batch 018 Loss: 0.0671\n",
      "Epoch 007 Batch 019 Loss: 0.0457\n",
      "Test Accuracy:  0.7830000519752502\n",
      "Epoch 008 Batch 000 Loss: 0.0419\n",
      "Epoch 008 Batch 001 Loss: 0.0346\n",
      "Epoch 008 Batch 002 Loss: 0.0575\n",
      "Epoch 008 Batch 003 Loss: 0.0315\n",
      "Epoch 008 Batch 004 Loss: 0.0598\n",
      "Epoch 008 Batch 005 Loss: 0.0500\n",
      "Epoch 008 Batch 006 Loss: 0.0514\n",
      "Epoch 008 Batch 007 Loss: 0.0508\n",
      "Epoch 008 Batch 008 Loss: 0.0500\n",
      "Epoch 008 Batch 009 Loss: 0.0383\n",
      "Epoch 008 Batch 010 Loss: 0.0414\n",
      "Epoch 008 Batch 011 Loss: 0.0506\n",
      "Epoch 008 Batch 012 Loss: 0.0463\n",
      "Epoch 008 Batch 013 Loss: 0.0411\n",
      "Epoch 008 Batch 014 Loss: 0.0341\n",
      "Epoch 008 Batch 015 Loss: 0.0690\n",
      "Epoch 008 Batch 016 Loss: 0.0392\n",
      "Epoch 008 Batch 017 Loss: 0.0498\n",
      "Epoch 008 Batch 018 Loss: 0.0659\n",
      "Epoch 008 Batch 019 Loss: 0.0507\n",
      "Test Accuracy:  0.7930000424385071\n",
      "Epoch 009 Batch 000 Loss: 0.0463\n",
      "Epoch 009 Batch 001 Loss: 0.0704\n",
      "Epoch 009 Batch 002 Loss: 0.0446\n",
      "Epoch 009 Batch 003 Loss: 0.0672\n",
      "Epoch 009 Batch 004 Loss: 0.0605\n",
      "Epoch 009 Batch 005 Loss: 0.0393\n",
      "Epoch 009 Batch 006 Loss: 0.0567\n",
      "Epoch 009 Batch 007 Loss: 0.0484\n",
      "Epoch 009 Batch 008 Loss: 0.0420\n",
      "Epoch 009 Batch 009 Loss: 0.0587\n",
      "Epoch 009 Batch 010 Loss: 0.0596\n",
      "Epoch 009 Batch 011 Loss: 0.0576\n",
      "Epoch 009 Batch 012 Loss: 0.0466\n",
      "Epoch 009 Batch 013 Loss: 0.0324\n",
      "Epoch 009 Batch 014 Loss: 0.0329\n",
      "Epoch 009 Batch 015 Loss: 0.0352\n",
      "Epoch 009 Batch 016 Loss: 0.0541\n",
      "Epoch 009 Batch 017 Loss: 0.0503\n",
      "Epoch 009 Batch 018 Loss: 0.0506\n",
      "Epoch 009 Batch 019 Loss: 0.0378\n",
      "Test Accuracy:  0.7800000309944153\n",
      "Epoch 010 Batch 000 Loss: 0.0756\n",
      "Epoch 010 Batch 001 Loss: 0.0385\n",
      "Epoch 010 Batch 002 Loss: 0.0437\n",
      "Epoch 010 Batch 003 Loss: 0.0484\n",
      "Epoch 010 Batch 004 Loss: 0.0544\n",
      "Epoch 010 Batch 005 Loss: 0.0543\n",
      "Epoch 010 Batch 006 Loss: 0.0410\n",
      "Epoch 010 Batch 007 Loss: 0.0514\n",
      "Epoch 010 Batch 008 Loss: 0.0375\n",
      "Epoch 010 Batch 009 Loss: 0.0541\n",
      "Epoch 010 Batch 010 Loss: 0.0262\n",
      "Epoch 010 Batch 011 Loss: 0.0433\n",
      "Epoch 010 Batch 012 Loss: 0.0459\n",
      "Epoch 010 Batch 013 Loss: 0.0331\n",
      "Epoch 010 Batch 014 Loss: 0.0554\n",
      "Epoch 010 Batch 015 Loss: 0.0549\n",
      "Epoch 010 Batch 016 Loss: 0.0468\n",
      "Epoch 010 Batch 017 Loss: 0.0440\n",
      "Epoch 010 Batch 018 Loss: 0.0488\n",
      "Epoch 010 Batch 019 Loss: 0.0372\n",
      "Test Accuracy:  0.7800000309944153\n",
      "Epoch 011 Batch 000 Loss: 0.0300\n",
      "Epoch 011 Batch 001 Loss: 0.0552\n",
      "Epoch 011 Batch 002 Loss: 0.0242\n",
      "Epoch 011 Batch 003 Loss: 0.0410\n",
      "Epoch 011 Batch 004 Loss: 0.1551\n",
      "Epoch 011 Batch 005 Loss: 0.0337\n",
      "Epoch 011 Batch 006 Loss: 0.0508\n",
      "Epoch 011 Batch 007 Loss: 0.0418\n",
      "Epoch 011 Batch 008 Loss: 0.0613\n",
      "Epoch 011 Batch 009 Loss: 0.0465\n",
      "Epoch 011 Batch 010 Loss: 0.0604\n",
      "Epoch 011 Batch 011 Loss: 0.0586\n",
      "Epoch 011 Batch 012 Loss: 0.0560\n",
      "Epoch 011 Batch 013 Loss: 0.0432\n",
      "Epoch 011 Batch 014 Loss: 0.0653\n",
      "Epoch 011 Batch 015 Loss: 0.0494\n",
      "Epoch 011 Batch 016 Loss: 0.0484\n",
      "Epoch 011 Batch 017 Loss: 0.0455\n",
      "Epoch 011 Batch 018 Loss: 0.0625\n",
      "Epoch 011 Batch 019 Loss: 0.0889\n",
      "Test Accuracy:  0.7670000195503235\n",
      "Epoch 012 Batch 000 Loss: 0.0769\n",
      "Epoch 012 Batch 001 Loss: 0.0234\n",
      "Epoch 012 Batch 002 Loss: 0.0289\n",
      "Epoch 012 Batch 003 Loss: 0.0566\n",
      "Epoch 012 Batch 004 Loss: 0.0517\n",
      "Epoch 012 Batch 005 Loss: 0.0809\n",
      "Epoch 012 Batch 006 Loss: 0.0639\n",
      "Epoch 012 Batch 007 Loss: 0.0910\n",
      "Epoch 012 Batch 008 Loss: 0.0595\n",
      "Epoch 012 Batch 009 Loss: 0.0463\n",
      "Epoch 012 Batch 010 Loss: 0.0467\n",
      "Epoch 012 Batch 011 Loss: 0.0428\n",
      "Epoch 012 Batch 012 Loss: 0.0416\n",
      "Epoch 012 Batch 013 Loss: 0.0540\n",
      "Epoch 012 Batch 014 Loss: 0.0472\n",
      "Epoch 012 Batch 015 Loss: 0.0689\n",
      "Epoch 012 Batch 016 Loss: 0.0725\n",
      "Epoch 012 Batch 017 Loss: 0.0456\n",
      "Epoch 012 Batch 018 Loss: 0.0908\n",
      "Epoch 012 Batch 019 Loss: 0.0428\n",
      "Test Accuracy:  0.7790000438690186\n",
      "Epoch 013 Batch 000 Loss: 0.0408\n",
      "Epoch 013 Batch 001 Loss: 0.0418\n",
      "Epoch 013 Batch 002 Loss: 0.0441\n",
      "Epoch 013 Batch 003 Loss: 0.0418\n",
      "Epoch 013 Batch 004 Loss: 0.0817\n",
      "Epoch 013 Batch 005 Loss: 0.0573\n",
      "Epoch 013 Batch 006 Loss: 0.0310\n",
      "Epoch 013 Batch 007 Loss: 0.0356\n",
      "Epoch 013 Batch 008 Loss: 0.0462\n",
      "Epoch 013 Batch 009 Loss: 0.0405\n",
      "Epoch 013 Batch 010 Loss: 0.0317\n",
      "Epoch 013 Batch 011 Loss: 0.0571\n",
      "Epoch 013 Batch 012 Loss: 0.0458\n",
      "Epoch 013 Batch 013 Loss: 0.0395\n",
      "Epoch 013 Batch 014 Loss: 0.0587\n",
      "Epoch 013 Batch 015 Loss: 0.0306\n",
      "Epoch 013 Batch 016 Loss: 0.0782\n",
      "Epoch 013 Batch 017 Loss: 0.1132\n",
      "Epoch 013 Batch 018 Loss: 0.0365\n",
      "Epoch 013 Batch 019 Loss: 0.0502\n",
      "Test Accuracy:  0.7790000438690186\n",
      "Epoch 014 Batch 000 Loss: 0.0450\n",
      "Epoch 014 Batch 001 Loss: 0.0546\n",
      "Epoch 014 Batch 002 Loss: 0.0416\n",
      "Epoch 014 Batch 003 Loss: 0.0499\n",
      "Epoch 014 Batch 004 Loss: 0.0490\n",
      "Epoch 014 Batch 005 Loss: 0.0349\n",
      "Epoch 014 Batch 006 Loss: 0.0438\n",
      "Epoch 014 Batch 007 Loss: 0.0469\n",
      "Epoch 014 Batch 008 Loss: 0.0463\n",
      "Epoch 014 Batch 009 Loss: 0.0455\n",
      "Epoch 014 Batch 010 Loss: 0.0427\n",
      "Epoch 014 Batch 011 Loss: 0.0638\n",
      "Epoch 014 Batch 012 Loss: 0.0395\n",
      "Epoch 014 Batch 013 Loss: 0.0414\n",
      "Epoch 014 Batch 014 Loss: 0.0418\n",
      "Epoch 014 Batch 015 Loss: 0.0688\n",
      "Epoch 014 Batch 016 Loss: 0.0375\n",
      "Epoch 014 Batch 017 Loss: 0.0372\n",
      "Epoch 014 Batch 018 Loss: 0.0591\n",
      "Epoch 014 Batch 019 Loss: 0.0515\n",
      "Test Accuracy:  0.7760000228881836\n",
      "Epoch 015 Batch 000 Loss: 0.0475\n",
      "Epoch 015 Batch 001 Loss: 0.0390\n",
      "Epoch 015 Batch 002 Loss: 0.0663\n",
      "Epoch 015 Batch 003 Loss: 0.0523\n",
      "Epoch 015 Batch 004 Loss: 0.0441\n",
      "Epoch 015 Batch 005 Loss: 0.0541\n",
      "Epoch 015 Batch 006 Loss: 0.0410\n",
      "Epoch 015 Batch 007 Loss: 0.0863\n",
      "Epoch 015 Batch 008 Loss: 0.0506\n",
      "Epoch 015 Batch 009 Loss: 0.0503\n",
      "Epoch 015 Batch 010 Loss: 0.0603\n",
      "Epoch 015 Batch 011 Loss: 0.0483\n",
      "Epoch 015 Batch 012 Loss: 0.0464\n",
      "Epoch 015 Batch 013 Loss: 0.0289\n",
      "Epoch 015 Batch 014 Loss: 0.0694\n",
      "Epoch 015 Batch 015 Loss: 0.0430\n",
      "Epoch 015 Batch 016 Loss: 0.0301\n",
      "Epoch 015 Batch 017 Loss: 0.0334\n",
      "Epoch 015 Batch 018 Loss: 0.0363\n",
      "Epoch 015 Batch 019 Loss: 0.0425\n",
      "Test Accuracy:  0.784000039100647\n",
      "Epoch 016 Batch 000 Loss: 0.0507\n",
      "Epoch 016 Batch 001 Loss: 0.0368\n",
      "Epoch 016 Batch 002 Loss: 0.0736\n",
      "Epoch 016 Batch 003 Loss: 0.0410\n",
      "Epoch 016 Batch 004 Loss: 0.0345\n",
      "Epoch 016 Batch 005 Loss: 0.0567\n",
      "Epoch 016 Batch 006 Loss: 0.0650\n",
      "Epoch 016 Batch 007 Loss: 0.0366\n",
      "Epoch 016 Batch 008 Loss: 0.0485\n",
      "Epoch 016 Batch 009 Loss: 0.0324\n",
      "Epoch 016 Batch 010 Loss: 0.0451\n",
      "Epoch 016 Batch 011 Loss: 0.0444\n",
      "Epoch 016 Batch 012 Loss: 0.0352\n",
      "Epoch 016 Batch 013 Loss: 0.0466\n",
      "Epoch 016 Batch 014 Loss: 0.0602\n",
      "Epoch 016 Batch 015 Loss: 0.0726\n",
      "Epoch 016 Batch 016 Loss: 0.0386\n",
      "Epoch 016 Batch 017 Loss: 0.0426\n",
      "Epoch 016 Batch 018 Loss: 0.0357\n",
      "Epoch 016 Batch 019 Loss: 0.0579\n",
      "Test Accuracy:  0.7780000567436218\n",
      "Epoch 017 Batch 000 Loss: 0.0422\n",
      "Epoch 017 Batch 001 Loss: 0.0589\n",
      "Epoch 017 Batch 002 Loss: 0.0398\n",
      "Epoch 017 Batch 003 Loss: 0.0369\n",
      "Epoch 017 Batch 004 Loss: 0.0355\n",
      "Epoch 017 Batch 005 Loss: 0.0501\n",
      "Epoch 017 Batch 006 Loss: 0.0777\n",
      "Epoch 017 Batch 007 Loss: 0.0479\n",
      "Epoch 017 Batch 008 Loss: 0.0322\n",
      "Epoch 017 Batch 009 Loss: 0.0824\n",
      "Epoch 017 Batch 010 Loss: 0.0400\n",
      "Epoch 017 Batch 011 Loss: 0.0383\n",
      "Epoch 017 Batch 012 Loss: 0.0380\n",
      "Epoch 017 Batch 013 Loss: 0.0851\n",
      "Epoch 017 Batch 014 Loss: 0.0529\n",
      "Epoch 017 Batch 015 Loss: 0.0447\n",
      "Epoch 017 Batch 016 Loss: 0.0369\n",
      "Epoch 017 Batch 017 Loss: 0.0468\n",
      "Epoch 017 Batch 018 Loss: 0.0643\n",
      "Epoch 017 Batch 019 Loss: 0.0345\n",
      "Test Accuracy:  0.7680000066757202\n",
      "Epoch 018 Batch 000 Loss: 0.0621\n",
      "Epoch 018 Batch 001 Loss: 0.0575\n",
      "Epoch 018 Batch 002 Loss: 0.0426\n",
      "Epoch 018 Batch 003 Loss: 0.0305\n",
      "Epoch 018 Batch 004 Loss: 0.0301\n",
      "Epoch 018 Batch 005 Loss: 0.0349\n",
      "Epoch 018 Batch 006 Loss: 0.0407\n",
      "Epoch 018 Batch 007 Loss: 0.0322\n",
      "Epoch 018 Batch 008 Loss: 0.0468\n",
      "Epoch 018 Batch 009 Loss: 0.0383\n",
      "Epoch 018 Batch 010 Loss: 0.0489\n",
      "Epoch 018 Batch 011 Loss: 0.0331\n",
      "Epoch 018 Batch 012 Loss: 0.1090\n",
      "Epoch 018 Batch 013 Loss: 0.0533\n",
      "Epoch 018 Batch 014 Loss: 0.0521\n",
      "Epoch 018 Batch 015 Loss: 0.0859\n",
      "Epoch 018 Batch 016 Loss: 0.0361\n",
      "Epoch 018 Batch 017 Loss: 0.0416\n",
      "Epoch 018 Batch 018 Loss: 0.0434\n",
      "Epoch 018 Batch 019 Loss: 0.0335\n",
      "Test Accuracy:  0.7780000567436218\n",
      "Epoch 019 Batch 000 Loss: 0.0378\n",
      "Epoch 019 Batch 001 Loss: 0.0431\n",
      "Epoch 019 Batch 002 Loss: 0.0522\n",
      "Epoch 019 Batch 003 Loss: 0.0587\n",
      "Epoch 019 Batch 004 Loss: 0.0800\n",
      "Epoch 019 Batch 005 Loss: 0.0402\n",
      "Epoch 019 Batch 006 Loss: 0.0346\n",
      "Epoch 019 Batch 007 Loss: 0.0483\n",
      "Epoch 019 Batch 008 Loss: 0.0526\n",
      "Epoch 019 Batch 009 Loss: 0.0465\n",
      "Epoch 019 Batch 010 Loss: 0.0384\n",
      "Epoch 019 Batch 011 Loss: 0.0710\n",
      "Epoch 019 Batch 012 Loss: 0.0517\n",
      "Epoch 019 Batch 013 Loss: 0.0555\n",
      "Epoch 019 Batch 014 Loss: 0.0336\n",
      "Epoch 019 Batch 015 Loss: 0.0490\n",
      "Epoch 019 Batch 016 Loss: 0.0425\n",
      "Epoch 019 Batch 017 Loss: 0.0257\n",
      "Epoch 019 Batch 018 Loss: 0.0686\n",
      "Epoch 019 Batch 019 Loss: 0.0447\n",
      "Test Accuracy:  0.7930000424385071\n"
     ]
    }
   ],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}